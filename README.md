# AzureOpenAI-Playground
#Para as entregas desse bootcamp, estarei inserindo as principais anotações que realizei durante as aulas dentro do arquivo Read.me

#1) Introdução: Para poder acessar os recursos desse curso, é necessário ter uma conta Azure válida, com acessos de Administrador e método de pagamento definido (inserindo cartão virtual ou por compra de créditos).

#2) Deploy: Em geral, é recomendado atrelar a inscrição à localidade East US ou East US 2, de forma a poder aproveitar melhores recursos e acesso à plataforma. També m é melhor deixar sem compartilhamento por rede, a fim de melhor controlar os gastos.

#3) Tudo certo, vamos ver nossa UI: Em geral, é melhor nÃo definir nenhuma regra no momento. É preciso pegar o link que representa o user endpoint. Há um catálogo de modelos de casos de uso da IA, igual ao visto na UI da AWS. Na opção chat, há o playground, que será utilizado como exemplo nesse capítulo. No Azure, é possível envir assistentes de código, áudios ao vivo, imagens e até conclusões obtidas pelo chat. Ao selecionar um modelo do catálogo, pode-se ver a opção de deploy, a linguagem de código da implantação e o SDK (Software Development Kit). O código é seguido tipo passo-a-passo. Tipo de implantação utilizada = Standard: pagamento feito por chamada à API, com limites de taxa mais baixos, sendo melhor para cargas de trabalho intermitentes, com baixo a médio volume. 

#4) O que é o Playground: É um local em que podemos realizar os testes com a IA, testando vários modelos, prompts, parâmetros, ou seja, um local em que desenvolvemos prompts para preparar nosso modelo final (cliente).

#5) Recursos do Playground: O aprendizado da Azure OpenAI segu uma linguagem estocástica, isto é, um processo que acaba por depender das leis do acaso. Por exemplo: o prompt "The quick brown" nem sempre retorna o restante do pangrama "jumps over the lazy dog", pois, a IA pode criar diferentes contextos daquele originalmente pretendido. Dessa forma, um mesmo prompt pode gerar mais de um resultado diferente.

#6) Tokenização: Quando se pensa na escolha do idioma em que se quer trabalhar com o chat, é preciso levar em conta o consumo de tokens, que é limitado e custoso ao desenvolvedor, afinal, enquanto "casa" utiliza 2 tokens, "home" utiliza apenas 1 token, ou seja, há uma desvantagem em utilizar o português na maioria dos cenários. Isso também é importante em estabelecer o texto com base em informações importantes, como o fato de emails poderem ser divididos em 1 token, ou como certos elementos da coodificação em Python utilizam menos tokens que palavras "normais". Também é importante destacar que emojis podem ocupar o espaço de até 7 tokens!! Curiosamente, o $ não conta Parâmetros a serem considerados: Mensagens anterioras incluídas (com isso, é possível determinar o tamanho do contexto que será utilizado pelo modelo na geração de respostas); Resposta máxima (define o número de limites que eu quero que o modelo me retorne); Temperatura (define o contraste de escolha nas opções); Top-p (restringe modelo para recorrer aos tokens mais prováveis, com base em probabilidade); Parar sequência (interrompe a sequência manualmente, por questões do próprio desenvolvedor); Penalidades por frequência e presença (impede o programa de repetir ou desviar do tópico do prompt). Uma boa sugestão de criação de modelos está no site da Hugging Face (https://huggingface.co/).

#7) System Message: Dando uma luz nessa sopa de parâmetros: Sistema de mensagens = ele define o contexto inicial do modelo, restringindo-o a um certo escopo de trabalho. Ele define o comportamento do modelo, utilizando algumas instruções base. Ele faz isso por shots (a maneira de como o LLM pode se portar), ou seja, vira uma questão de prompt engineering. Para fazer um bom System Message -> Zero-Shot = sem exemplo, obtém resposta direta, baseada apenas no prompt. A formulação é feita pelos limites da Resposta máxima do modelo e Mensagens anteriores incluídas -> eles podem otimizar custos de utilização computacional e de uso de tokens, porém, há o encurtamento do texto por corte.

#8) Temperatura x Top-P: Esses dois parâmetros auxiliam na melhoria da criatividade e na criação de respostas diferentes. Temperatura = pega as palavras durante a criação do texto e controla o quão random são as ligações. Vai de 0 (muito previsível, determinístico) a 1 (muito improvável, nonsense). Altas temperaturas são indicadas para criação de ideias. Top-P = determina quais palavras podem ser usadas na possível escolha, como um filtro probabilístico: 0.1 (só considera o primeiro eixo de palavras prováveis até dar 10% das possibilidades) versus 0.9 (considera muitas palavras até dar o eixo de 90% das possibilidades). Atenção: quase nunca se utiliza ambos os parâmetros -> ou usa um, ou usa outro !! 

#9) Frequence Penalty x Presence Penalty: Frequence = determina a variação vocabular, para reprimir uma repetição de tokens, fazendo bom uso de seu limite financeiro. Presence = oune pelo desvio de tópico pela IA em relação ao contexto desejado.

#10) Multimodalidade: Significa que pode gerar mais do que texto. Por exemplo, no Dall-E, são geradas imagens, sendo necessária uma boa estruturação: deve ter clareza, especificidade, contexti e estrutura. É altamente recomendado utilizar as saídas do Dall-E apenas como uma prototipagem de uma ideia ou projeto, e não para essas imagens serem incorporadas no produto final! Uma boa prática é começar com defaults -> ajustar um parâmetro por vez -> documentar os resultados -> realizar as devidas alterações com base em feedbacks.

#11) Hands-On: Explorando o Playground - Parte 1: No exemplo visto sobre completar uma história de ficção científica -> um Top-P alto com Temperatura baixa gera algo criativo, porém limitado, ou seja, há uma variedade controlada; um Top-P baixo com uma Temperatura alta gera um retorno de criatividade limitada; quando ambos estão baixos ocorre uma hiper previsibilidade da saída do modelo; e quando ambos estão altos, existe uma criatividade máxima. No exemplo visto sobre interpretar headlines e justificar sua sepração por tópicos: o zero-shot, embora tenha conseguido compreender o comando dado pelo prompt, tem dificuldade com formatação -> nesses casos, a correção é feita pela descrição da saída desejada no default case.

#11) Hands-On - Parte 2: Em  Storage Accounts -> local em que inserimos as informações do blob desejado, onde, nesse caso, será utilizado o storage básico com demais padrões default da Microsoft. Na página inicial do Azure -> ainda em Storage Accounts -> Uplad blob -> cria-se um contâiner com o nome desejado. No Índice de busca é criado os Indexes, usando o Tier S, pois, é possível a utilização da casa de milhões de blobs. Em adicionar fonte de dados -> Azure Blob Storage -> criamos o índice -> inserindo, no local indexador, quantas vezes será inseirda documentação no blob (definiu-se por uma única vez, com editais de processo seletivo da Marinha). Define-se também se é pesquisa por palavras-chave ou por semântica -> escolhendo também o tamanho da tokenização. Para podermos avaliar o chat bot, é melhor termos em mãos um "prompt prompt", para deixar o bot pronto. Cuidado 1: se o bot pegar o prompt, reestruturá-lo e jogar de novo na conversa, pode ocorrer o famoso Erro 429, ond o limite de TPM não foi respeitado em relação ao tempo. Cuidado 2: sempre lembrar de inserir os limites no chat Client em conta com o limite de TPM. Cuidado 3: no Dall-E, é importante ter uma descrição precisa da imagem a ser gerada, sendo possível até ver o código gerado pelo prompt dentro da própria plataforma da Azure!
